from PIL import Image
import cv2

# INTER_LINEARì´ ì—†ìœ¼ë©´ ëŒ€ì²´ê°’ ì§ì ‘ ì„¤ì • (ë³´í†µ 1)
if not hasattr(cv2, 'INTER_LINEAR'):
    cv2.INTER_LINEAR = 1

import numpy as np
from typing import List, Tuple, Dict, Any

# ì´ íŒŒì¼ì˜ í•¨ìˆ˜ë“¤ì€ PIL Imageì™€ OpenCV ê°ì²´ë¥¼ ì£¼ë¡œ ë‹¤ë£¨ë©°, 
# ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ëª¨ë“ˆì— ëŒ€í•œ ì§ì ‘ì ì¸ ì˜ì¡´ì„±ì€ í˜„ì¬ ì—†ì–´ ë³´ì…ë‹ˆë‹¤.
# ë§Œì•½ DetectedArea ê°™ì€ íƒ€ì…ì„ ì—¬ê¸°ì„œë„ ì‚¬ìš©í•œë‹¤ë©´ from ..data_structures import DetectedArea ì¶”ê°€ í•„ìš”.

def enhance_and_find_contours_for_lines(pil_image: Image.Image) -> List[Tuple[int, int, int, int]]:
    """
    ì´ë¯¸ì§€ì—ì„œ ìˆ˜í‰ì„ ì„ ê²€ì¶œí•˜ì—¬ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    ì£¼ë¡œ í‘œë‚˜ ë¬¸ì„œì—ì„œ í–‰ì„ êµ¬ë¶„í•˜ëŠ” ìˆ˜í‰ì„ ë“¤ì„ ì°¾ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.
    CLAHE ëŒ€ë¹„ ê°œì„ , ëª¨í´ë¡œì§€ ì—°ì‚°ì„ í†µí•œ ìˆ˜í‰ì„  ê°•í™”, ì»¨íˆ¬ì–´ ê²€ì¶œì„ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        pil_image (Image.Image): ì…ë ¥ PIL ì´ë¯¸ì§€ ê°ì²´
        
    Returns:
        List[Tuple[int, int, int, int]]: ê²€ì¶œëœ ìˆ˜í‰ì„ ë“¤ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸
                                        ê° íŠœí”Œì€ (x, y, width, height) í˜•ì‹
    
    Processing Steps:
        1. RGB í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ë° OpenCV ë°°ì—´ ë³€í™˜
        2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        3. CLAHEë¥¼ ì‚¬ìš©í•œ ëŒ€ë¹„ ê°œì„  (clipLimit=2.0, tileGridSize=(8,8))
        4. ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë…¸ì´ì¦ˆ ì œê±° (kernel_size=(5,5))
        5. OTSU ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ì´ì§„í™” (ë°˜ì „ëœ ì´ì§„ ì´ë¯¸ì§€)
        6. ìˆ˜í‰ ì»¤ë„ì„ ì‚¬ìš©í•œ ëª¨í´ë¡œì§€ ì˜¤í”„ë‹ìœ¼ë¡œ ìˆ˜í‰ì„  ê°•í™”
        7. ì»¨íˆ¬ì–´ ê²€ì¶œ ë° í¬ê¸° í•„í„°ë§
        
    Filtering Criteria:
        - ìµœì†Œ ë„ˆë¹„: ì´ë¯¸ì§€ ë„ˆë¹„ì˜ 1/6 ì´ìƒ
        - ìµœëŒ€ ë†’ì´: 20í”½ì…€ ì´í•˜
        - ìˆ˜í‰ ì»¤ë„ í¬ê¸°: max(15, ì´ë¯¸ì§€ë„ˆë¹„/3)
    """
    # 1. ì´ë¯¸ì§€ í˜•ì‹ ì •ê·œí™”: RGBë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ëœ ì²˜ë¦¬ ë³´ì¥
    if pil_image.mode != 'RGB':
        pil_image = pil_image.convert('RGB')
    cv_image = np.array(pil_image)
    
    # 2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜: ìƒ‰ìƒ ì •ë³´ ì œê±°í•˜ì—¬ êµ¬ì¡°ì  íŠ¹ì§•ì— ì§‘ì¤‘
    gray = cv2.cvtColor(cv_image, cv2.COLOR_RGB2GRAY)
    
    # 3. CLAHE (Contrast Limited Adaptive Histogram Equalization) ì ìš©
    # - ì§€ì—­ì  ëŒ€ë¹„ ê°œì„ ìœ¼ë¡œ ìˆ˜í‰ì„  ì‹œê°ì  ê°•í™”
    # - clipLimit=2.0: ê³¼ë„í•œ ëŒ€ë¹„ ì¦ê°€ ë°©ì§€
    # - tileGridSize=(8,8): 8x8 íƒ€ì¼ ë‹¨ìœ„ë¡œ ì ì‘ì  íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    enhanced = clahe.apply(gray)
    
    # 4. ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬: ë…¸ì´ì¦ˆ ì œê±° ë° ì´ë¯¸ì§€ í‰í™œí™”
    # - kernel_size=(5,5): ì¤‘ê°„ ì •ë„ì˜ ë¸”ëŸ¬ë§ìœ¼ë¡œ ì„¸ë¶€ ë…¸ì´ì¦ˆ ì œê±°
    blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)
    
    # 5. ì´ì§„í™”: OTSU ë°©ë²•ìœ¼ë¡œ ìë™ ì„ê³„ê°’ ê²°ì •
    # - THRESH_BINARY_INV: ì „ê²½(ìˆ˜í‰ì„ )ì„ í°ìƒ‰(255)ìœ¼ë¡œ, ë°°ê²½ì„ ê²€ì€ìƒ‰(0)ìœ¼ë¡œ
    # - OTSU: ì´ë¯¸ì§€ íˆìŠ¤í† ê·¸ë¨ì„ ë¶„ì„í•˜ì—¬ ìµœì  ì„ê³„ê°’ ìë™ ê²°ì •
    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    
    # 6. ìˆ˜í‰ì„  ê²€ì¶œì„ ìœ„í•œ ëª¨í´ë¡œì§€ ì—°ì‚°
    # - ìˆ˜í‰ ì»¤ë„ í¬ê¸°: ì´ë¯¸ì§€ ë„ˆë¹„ì— ë¹„ë¡€í•˜ë˜ ìµœì†Œ 15í”½ì…€ ë³´ì¥
    # - ê°€ë¡œë¡œ ê¸´ ì§ì‚¬ê°í˜• ì»¤ë„ë¡œ ìˆ˜í‰ êµ¬ì¡° ê°•í™”
    horizontal_size = max(15, cv_image.shape[1] // 3)
    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (horizontal_size, 1))
    
    # - ëª¨í´ë¡œì§€ ì˜¤í”„ë‹: ì¹¨ì‹ í›„ íŒ½ì°½ìœ¼ë¡œ ìˆ˜í‰ì„ ë§Œ ë‚¨ê¸°ê³  ë‹¤ë¥¸ êµ¬ì¡° ì œê±°
    # - iterations=1: í•œ ë²ˆì˜ ì˜¤í”„ë‹ ì—°ì‚°ìœ¼ë¡œ ì ì ˆí•œ í•„í„°ë§
    horizontal_lines_mask = cv2.morphologyEx(binary, cv2.MORPH_OPEN, horizontal_kernel, iterations=1)
    
    # 7. ì»¨íˆ¬ì–´ ê²€ì¶œ: ìˆ˜í‰ì„ ë“¤ì˜ ì™¸ê³½ì„  ì¶”ì¶œ
    # - RETR_EXTERNAL: ê°€ì¥ ë°”ê¹¥ìª½ ì»¨íˆ¬ì–´ë§Œ ê²€ì¶œ
    # - CHAIN_APPROX_SIMPLE: ì»¨íˆ¬ì–´ ì  ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    contours, _ = cv2.findContours(horizontal_lines_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # 8. ê²€ì¶œëœ ì»¨íˆ¬ì–´ í•„í„°ë§ ë° ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ
    detected_lines_bboxes = []
    min_line_width = cv_image.shape[1] // 6  # ìµœì†Œ ë„ˆë¹„: ì´ë¯¸ì§€ ë„ˆë¹„ì˜ 1/6
    
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        
        # í•„í„°ë§ ì¡°ê±´:
        # - ë„ˆë¹„ê°€ ìµœì†Œ ê¸°ì¤€ ì´ìƒ: ì‹¤ì œ ìˆ˜í‰ì„ ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ê¸¸ì´
        # - ë†’ì´ê°€ 20í”½ì…€ ì´í•˜: ì„ ì˜ íŠ¹ì„±ìƒ ì–‡ì•„ì•¼ í•¨ (í…ìŠ¤íŠ¸ë‚˜ ë‹¤ë¥¸ ê°ì²´ ì œì™¸)
        if w >= min_line_width and h <= 20:
             detected_lines_bboxes.append((x, y, w, h))
             
    return detected_lines_bboxes

def enhance_and_find_contours_for_lines_v2(
    pil_image: Image.Image,
    kernel_size_ratio: float = 0.5,
    min_width_ratio: float = 0.2,
    max_height: int = 15,
    merge_threshold: int = 20,
    clahe_clip_limit: float = 2.0,
    use_adaptive_kernel: bool = True
) -> List[Tuple[int, int, int, int]]:
    """
    ê°œì„ ëœ ìˆ˜í‰ì„  ê²€ì¶œ í•¨ìˆ˜ (v2)
    
    ê¸°ì¡´ í•¨ìˆ˜ì™€ line_detection.pyì˜ ê²€ì¦ëœ ë¡œì§ì„ ê²°í•©í•˜ì—¬ ë” ë‚˜ì€ ì„±ëŠ¥ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
    ë‹¤ì–‘í•œ ë¬¸ì„œ íƒ€ì…ê³¼ ì´ë¯¸ì§€ í’ˆì§ˆì— ì ì‘í•  ìˆ˜ ìˆë„ë¡ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì • ê°€ëŠ¥í•˜ê²Œ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.
    
    Args:
        pil_image (Image.Image): ì…ë ¥ PIL ì´ë¯¸ì§€ ê°ì²´
        kernel_size_ratio (float): ìˆ˜í‰ ì»¤ë„ í¬ê¸° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.5, ì´ë¯¸ì§€ ë„ˆë¹„ì˜ 50%)
        min_width_ratio (float): ìµœì†Œ ì„  ë„ˆë¹„ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.2, ì´ë¯¸ì§€ ë„ˆë¹„ì˜ 20%)  
        max_height (int): ìˆ˜í‰ì„ ìœ¼ë¡œ ì¸ì •í•  ìµœëŒ€ ë†’ì´ (ê¸°ë³¸ê°’: 15í”½ì…€)
        merge_threshold (int): ê°€ê¹Œìš´ yì¢Œí‘œ ë³‘í•© ì„ê³„ê°’ (ê¸°ë³¸ê°’: 20í”½ì…€)
        clahe_clip_limit (float): CLAHE í´ë¦½ í•œê³„ê°’ (ê¸°ë³¸ê°’: 2.0)
        use_adaptive_kernel (bool): ì ì‘ì  ì»¤ë„ í¬ê¸° ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        
    Returns:
        List[Tuple[int, int, int, int]]: ê²€ì¶œëœ ìˆ˜í‰ì„ ë“¤ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸
                                        ê° íŠœí”Œì€ (x, y, width, height) í˜•ì‹
    
    Improvements over v1:
        1. íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ëŠ¥: ë‹¤ì–‘í•œ ë¬¸ì„œ íƒ€ì…ì— ì ì‘
        2. ì ì‘ì  ì»¤ë„ í¬ê¸°: ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¥¸ ë™ì  ì¡°ì •
        3. í–¥ìƒëœ í•„í„°ë§: line_detection.pyì˜ ê²€ì¦ëœ ê¸°ì¤€ ì ìš©
        4. ì¢Œí‘œ ë³‘í•© ë¡œì§: ì¤‘ë³µ ì„  ì œê±° ê°œì„ 
        5. ì„±ëŠ¥ ìµœì í™”: ë¶ˆí•„ìš”í•œ ì—°ì‚° ì œê±°
        
    Processing Algorithm:
        1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (RGB ì •ê·œí™”, ê·¸ë ˆì´ìŠ¤ì¼€ì¼)
        2. ì ì‘ì  CLAHE ëŒ€ë¹„ ê°œì„  
        3. ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ë…¸ì´ì¦ˆ ì œê±°
        4. OTSU ì´ì§„í™”
        5. ë™ì  í¬ê¸° ìˆ˜í‰ ì»¤ë„ ì ìš©
        6. ëª¨í´ë¡œì§€ ì˜¤í”„ë‹ìœ¼ë¡œ ìˆ˜í‰ì„  ì¶”ì¶œ
        7. ì§€ëŠ¥í˜• ì»¨íˆ¬ì–´ í•„í„°ë§
        8. ì¤‘ë³µ ì œê±° ë° ë³‘í•©
    """
    # 1. ì…ë ¥ ê²€ì¦ ë° ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    if pil_image.mode != 'RGB':
        pil_image = pil_image.convert('RGB')
    cv_image = np.array(pil_image)
    
    # ì´ë¯¸ì§€ í¬ê¸° ê²€ì¦
    img_height, img_width = cv_image.shape[:2]
    if img_height < 10 or img_width < 10:
        return []  # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ëŠ” ì²˜ë¦¬ ë¶ˆê°€
    
    # 2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
    gray = cv2.cvtColor(cv_image, cv2.COLOR_RGB2GRAY)
    
    # 3. í–¥ìƒëœ CLAHE ì ìš© (line_detection.py ê²€ì¦ëœ íŒŒë¼ë¯¸í„° ì ìš©)
    clahe = cv2.createCLAHE(clipLimit=clahe_clip_limit, tileGridSize=(8,8))
    enhanced = clahe.apply(gray)
    
    # 4. ì ì‘ì  ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ (ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¥¸ ì»¤ë„ ì¡°ì •)
    blur_size = max(3, min(7, img_width // 200))  # ì´ë¯¸ì§€ í¬ê¸°ì— ë¹„ë¡€í•œ ë¸”ëŸ¬ ì»¤ë„
    if blur_size % 2 == 0:  # í™€ìˆ˜ë¡œ ë§ì¶”ê¸°
        blur_size += 1
    blurred = cv2.GaussianBlur(enhanced, (blur_size, blur_size), 0)
    
    # 5. OTSU ì´ì§„í™” (ê²€ì¦ëœ ë°©ì‹ ìœ ì§€)
    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    
    # 6. ë™ì  ìˆ˜í‰ ì»¤ë„ í¬ê¸° ê³„ì‚° (v1ê³¼ line_detection.py ë°©ì‹ ê²°í•©)
    if use_adaptive_kernel:
        # ì ì‘ì  ë°©ì‹: ì´ë¯¸ì§€ í¬ê¸°ì™€ ë¹„ìœ¨ì„ ê³ ë ¤
        base_kernel_size = int(img_width * kernel_size_ratio)
        horizontal_size = max(15, min(base_kernel_size, img_width // 2))
    else:
        # ê³ ì • ë°©ì‹: line_detection.py ìŠ¤íƒ€ì¼
        horizontal_size = img_width // 2
    
    # 7. ìˆ˜í‰ì„  ê°•í™” ëª¨í´ë¡œì§€ ì—°ì‚°
    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (horizontal_size, 1))
    horizontal_lines_mask = cv2.morphologyEx(binary, cv2.MORPH_OPEN, horizontal_kernel, iterations=1)
    
    # 8. ì»¨íˆ¬ì–´ ê²€ì¶œ
    contours, _ = cv2.findContours(horizontal_lines_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # 9. í–¥ìƒëœ í•„í„°ë§ (line_detection.pyì˜ ê²€ì¦ëœ ê¸°ì¤€ ì ìš©)
    detected_lines_bboxes = []
    min_line_width = int(img_width * min_width_ratio)  # íŒŒë¼ë¯¸í„°í™”ëœ ìµœì†Œ ë„ˆë¹„
    
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        
        # ê°œì„ ëœ í•„í„°ë§ ì¡°ê±´:
        # 1. ìµœì†Œ ë„ˆë¹„ ê²€ì¦ (line_detection.py ìŠ¤íƒ€ì¼: ë” ê´€ëŒ€í•œ ê¸°ì¤€)
        # 2. ìµœëŒ€ ë†’ì´ ê²€ì¦ (ë” ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ)
        # 3. ê°€ë¡œì„¸ë¡œ ë¹„ìœ¨ ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€: ì‹¤ì œ ì„ ì¸ì§€ í™•ì¸)
        if w >= min_line_width and h <= max_height:
            aspect_ratio = w / h if h > 0 else float('inf')
            # ê°€ë¡œì„¸ë¡œ ë¹„ìœ¨ì´ ìµœì†Œ 3:1 ì´ìƒì´ì–´ì•¼ ìˆ˜í‰ì„ ìœ¼ë¡œ ì¸ì •
            if aspect_ratio >= 3.0:
                detected_lines_bboxes.append((x, y, w, h))
    
    # 10. ì¤‘ë³µ ì œê±° ë° Yì¢Œí‘œ ë³‘í•© (line_detection.py ë¡œì§ ê°œì„ )
    if not detected_lines_bboxes:
        return []
    
    # Yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    detected_lines_bboxes.sort(key=lambda bbox: bbox[1])
    
    # ê°€ê¹Œìš´ Yì¢Œí‘œì˜ ì„ ë“¤ì„ ë³‘í•©
    merged_lines = []
    current_line = detected_lines_bboxes[0]
    
    for next_line in detected_lines_bboxes[1:]:
        curr_y = current_line[1]
        next_y = next_line[1]
        
        # ë³‘í•© ì¡°ê±´: Yì¢Œí‘œ ì°¨ì´ê°€ ì„ê³„ê°’ ì´ë‚´
        if abs(next_y - curr_y) <= merge_threshold:
            # ë” ê¸´ ì„ ì„ ì„ íƒí•˜ì—¬ ë³‘í•©
            if next_line[2] > current_line[2]:  # width ë¹„êµ
                current_line = next_line
        else:
            # ë³‘í•©í•˜ì§€ ì•Šê³  í˜„ì¬ ì„  ì €ì¥
            merged_lines.append(current_line)
            current_line = next_line
    
    # ë§ˆì§€ë§‰ ì„  ì¶”ê°€
    merged_lines.append(current_line)
    
    # 11. ìµœì¢… ê²€ì¦ ë° ì •ë ¬
    final_lines = []
    for line in merged_lines:
        x, y, w, h = line
        # ìµœì¢… ê²€ì¦: ë„ˆë¬´ ì‘ê±°ë‚˜ ì´ìƒí•œ ì„  ì œê±°
        if w >= min_line_width * 0.8 and h <= max_height and w >= h * 2:
            final_lines.append(line)
    
    # Xì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì½ê¸° ìˆœì„œ ê³ ë ¤)
    final_lines.sort(key=lambda bbox: (bbox[1], bbox[0]))  # Yì¢Œí‘œ ìš°ì„ , Xì¢Œí‘œ ë³´ì¡°
    
    return final_lines

def crop_between_lines(
    pil_image: Image.Image, 
    detected_lines_bboxes: List[Tuple[int,int,int,int]]
) -> List[Dict[str, Any]]: # [{'image_obj': Image, 'y_top_in_area': int, 'y_bottom_in_area': int}]
    line_y_coords = [0]
    for _, y_line, _, h_line in detected_lines_bboxes:
        line_y_coords.extend([y_line, y_line + h_line])
    line_y_coords.append(pil_image.height)
    line_y_coords = sorted(list(set(line_y_coords)))

    merged_y = []
    if line_y_coords:
        i = 0
        while i < len(line_y_coords):
            current = line_y_coords[i]
            j = i + 1
            while j < len(line_y_coords) and line_y_coords[j] - current < 25:
                j += 1
            merged_y.append(current)
            i = j
        line_y_coords = merged_y

    line_cropped_outputs: List[Dict[str, Any]] = []
    for i in range(len(line_y_coords) - 1):
        y_start, y_end = line_y_coords[i], line_y_coords[i+1]
        height = y_end - y_start
        if height < 15:
            continue
        
        # ìœ„ì•„ë˜ë¡œ 3í”½ì…€ì”© ì—¬ìœ  ê³µê°„ ì¶”ê°€ (ì´ë¯¸ì§€ ê²½ê³„ ì²´í¬)
        y_start_shrink = max(0, y_start + 3)
        y_end_shrink = min(pil_image.height, y_end - 3)
        
        cropped_pil = pil_image.crop((5, y_start_shrink, pil_image.width-15, y_end_shrink)) # ìƒí•˜ì¢Œìš° ì¼ë¶€ í”½ì…€ì„ ì˜ë¼ë‚´ì„œ í‘œì˜ ì„ ì´ ì¡íˆì§€ ì•Šë„ë¡ í•¨ 0610 ë‹¤í›ˆ
        line_cropped_outputs.append({
            'image_obj': cropped_pil, 
            'y_top_in_area': y_start,  # ì›ë˜ y ì¢Œí‘œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            'y_bottom_in_area': y_end  # ì›ë˜ y ì¢Œí‘œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        })
    return line_cropped_outputs

def preprocess_line_image_for_text_contours(line_pil_image: Image.Image) -> List[np.ndarray]:
    if line_pil_image.mode != 'RGB':
        line_pil_image = line_pil_image.convert('RGB')
    cv_image = np.array(line_pil_image)
    if cv_image.shape[0] < 5 or cv_image.shape[1] < 5: return []

    # split_and_recognize_single_digits.pyì™€ ë™ì¼í•œ ë‹¨ìˆœí•˜ê³  íš¨ê³¼ì ì¸ ë°©ë²• ì‚¬ìš©
    gray = cv2.cvtColor(cv_image, cv2.COLOR_RGB2GRAY)
    _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    return contours

def merge_contours_and_crop_text_pil(
    line_pil_image: Image.Image, 
    contours: List[np.ndarray],
    merge_distance_threshold: int = 100,
    padding: int = 5
) -> List[Dict[str, Any]]: # [{'image_obj': Image, 'x_in_line': int, 'y_in_line': int}]
    bounding_boxes_initial: List[Dict[str, Any]] = []
    img_width = line_pil_image.width
    img_height = line_pil_image.height
    
    # ë¹ˆ ë°•ìŠ¤ í•„í„°ë§ì„ ìœ„í•œ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë°°ì—´
    line_np_array = np.array(line_pil_image.convert('L'))
    
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        
        # ë§Œì•½ wê°€ ì›ë³¸ ì´ë¯¸ì§€ì˜ 95% ì´ìƒì´ë©´ ë³‘í•© ëŒ€ìƒì—ì„œ ì œì™¸
        if w > 0.95 * img_width:
            continue

        # ë§Œì•½ ë„ˆë¬´ ì‘ì€ ë°•ìŠ¤ë©´ ë³‘í•© ëŒ€ìƒì—ì„œ ì œì™¸
        if w < 2 or h < 10:
            continue
        
        # ê°€ë¡œì„ (í‘œì˜ ì¼ë¶€) ì œê±° - ê°€ë¡œì„¸ë¡œ ë¹„ìœ¨ì´ ë„ˆë¬´ í° ê²ƒë“¤ ì œê±°
        aspect_ratio = w / h
        if aspect_ratio > 1.8:  # ê°€ë¡œê°€ ì„¸ë¡œì˜ 6ë°° ì´ìƒì´ë©´ ê°€ë¡œì„ ìœ¼ë¡œ ê°„ì£¼
            continue

        # if aspect_ratio < 0.5:
        #     continue
        
        # ë¹ˆ ë°•ìŠ¤ë‚˜ í…Œë‘ë¦¬ë§Œ ìˆëŠ” ì˜ì—­ ì œê±° (ê³¼ê°í•˜ê²Œ ì—„ê²©í•¨)
        roi = line_np_array[y:y+h, x:x+w]
        if roi.size > 0:
            # ê²€ì€ í”½ì…€ ë¹„ìœ¨ ê³„ì‚°
            dark_pixels = np.sum(roi < 150)  # 180 â†’ 120ìœ¼ë¡œ ê³¼ê°í•˜ê²Œ ì—„ê²©
            total_pixels = roi.size
            dark_ratio = dark_pixels / total_pixels
            
            # ë¹ˆ ì˜ì—­ ì œì™¸ ì¡°ê±´ì„ ê³¼ê°í•˜ê²Œ ì—„ê²©í•˜ê²Œ
            if dark_ratio < 0.04:  # 1.5% â†’ 5%ë¡œ ê³¼ê°í•˜ê²Œ ì—„ê²©
                continue
        
        bounding_boxes_initial.append({'x':x, 'y':y, 'w':w, 'h':h, 'xc': x + w/2, 'yc': y + h/2, 'merged': False})

    if not bounding_boxes_initial:
        return []

    bounding_boxes_initial.sort(key=lambda b: b['x'])
    
    merged_boxes_final: List[Dict[str, Any]] = []
    current_merged_box = None

    for i, box_info in enumerate(bounding_boxes_initial):
        if not current_merged_box:
            current_merged_box = box_info.copy()
            current_merged_box['merged_count'] = 1
            continue

        prev_xc, prev_yc = current_merged_box['xc'], current_merged_box['yc']
        curr_xc, curr_yc = box_info['xc'], box_info['yc']
        
        dist_x_centers = abs(curr_xc - prev_xc)
        y_overlap = max(current_merged_box['y'], box_info['y']) < min(current_merged_box['y'] + current_merged_box['h'], box_info['y'] + box_info['h'])

        if dist_x_centers < merge_distance_threshold and y_overlap:
            new_x = min(current_merged_box['x'], box_info['x'])
            new_y = min(current_merged_box['y'], box_info['y'])
            new_x_plus_w = max(current_merged_box['x'] + current_merged_box['w'], box_info['x'] + box_info['w'])
            new_y_plus_h = max(current_merged_box['y'] + current_merged_box['h'], box_info['y'] + box_info['h'])
            
            current_merged_box['x'] = new_x
            current_merged_box['y'] = new_y
            current_merged_box['w'] = new_x_plus_w - new_x
            current_merged_box['h'] = new_y_plus_h - new_y
            current_merged_box['xc'] = current_merged_box['x'] + current_merged_box['w'] / 2
            current_merged_box['yc'] = current_merged_box['y'] + current_merged_box['h'] / 2
            current_merged_box['merged_count'] +=1
        else:
            merged_boxes_final.append(current_merged_box)
            current_merged_box = box_info.copy()
            current_merged_box['merged_count'] = 1
            
    if current_merged_box:
        merged_boxes_final.append(current_merged_box)

    final_text_crop_outputs: List[Dict[str, Any]] = []
    for box_data in merged_boxes_final:
        x, y, w, h = box_data['x'], box_data['y'], box_data['w'], box_data['h']
        original_w = w
        original_h = h
        
        x_p = max(0, x - padding)
        y_p = max(0, y - padding)
        r_p = min(line_pil_image.width, x + w + padding)
        b_p = min(line_pil_image.height, y + h + padding)

        if r_p <= x_p or b_p <= y_p: continue

        text_crop_pil = line_pil_image.crop((x_p, y_p, r_p, b_p))
        
        target_w, target_h = text_crop_pil.width, text_crop_pil.height
        
        square_size = max(target_w, target_h)
        
        square_canvas_pil = Image.new('RGB', (square_size, square_size), (255, 255, 255))
        paste_x = (square_size - target_w) // 2
        paste_y = (square_size - target_h) // 2
        square_canvas_pil.paste(text_crop_pil, (paste_x, paste_y))
        
        final_text_crop_outputs.append({
            'image_obj': square_canvas_pil, 
            'x_in_line': x,
            'y_in_line': y,
            'original_w': original_w,
            'original_h': original_h
        })
        
    final_text_crop_outputs.sort(key=lambda item: item['x_in_line'])
    return final_text_crop_outputs 

def visualize_line_detection_comparison(
    pil_image: Image.Image, 
    save_path: str = None,
    test_both_versions: bool = True
) -> Dict[str, Any]:
    """
    ìˆ˜í‰ì„  ê²€ì¶œ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê³  v1, v2 ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        pil_image: ì…ë ¥ PIL ì´ë¯¸ì§€
        save_path: ì €ì¥í•  ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        test_both_versions: v1, v2 ëª¨ë‘ í…ŒìŠ¤íŠ¸í• ì§€ ì—¬ë¶€
    
    Returns:
        Dict containing comparison results and statistics
    """
    import matplotlib.pyplot as plt
    import time
    
    results = {
        'v1': {'lines': [], 'time': 0, 'count': 0},
        'v2': {'lines': [], 'time': 0, 'count': 0}
    }
    
    # V1 í…ŒìŠ¤íŠ¸
    start_time = time.time()
    v1_lines = enhance_and_find_contours_for_lines(pil_image)
    results['v1']['time'] = time.time() - start_time
    results['v1']['lines'] = v1_lines
    results['v1']['count'] = len(v1_lines)
    
    if test_both_versions:
        # V2 í…ŒìŠ¤íŠ¸ (ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°)
        start_time = time.time()
        v2_lines = enhance_and_find_contours_for_lines_v2(pil_image)
        results['v2']['time'] = time.time() - start_time
        results['v2']['lines'] = v2_lines
        results['v2']['count'] = len(v2_lines)
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 3 if test_both_versions else 2, figsize=(15, 5))
    
    # ì›ë³¸ ì´ë¯¸ì§€
    axes[0].imshow(pil_image)
    axes[0].set_title('Original Image')
    axes[0].axis('off')
    
    # V1 ê²°ê³¼
    img_v1 = np.array(pil_image.copy())
    for x, y, w, h in v1_lines:
        cv2.rectangle(img_v1, (x, y), (x+w, y+h), (0, 255, 0), 2)
    axes[1].imshow(img_v1)
    axes[1].set_title(f'V1: {results["v1"]["count"]} lines\nTime: {results["v1"]["time"]:.3f}s')
    axes[1].axis('off')
    
    if test_both_versions:
        # V2 ê²°ê³¼
        img_v2 = np.array(pil_image.copy())
        for x, y, w, h in v2_lines:
            cv2.rectangle(img_v2, (x, y), (x+w, y+h), (255, 0, 0), 2)
        axes[2].imshow(img_v2)
        axes[2].set_title(f'V2: {results["v2"]["count"]} lines\nTime: {results["v2"]["time"]:.3f}s')
        axes[2].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ë¹„êµ ê²°ê³¼ ì €ì¥: {save_path}")
    
    plt.show()
    
    return results

def debug_text_crop_pipeline(
    pil_image: Image.Image,
    save_dir: str = "/tmp/debug_crops",
    use_v2: bool = True
) -> Dict[str, Any]:
    """
    í…ìŠ¤íŠ¸ í¬ë¡­ íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë¥¼ ë””ë²„ê·¸í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        pil_image: ì…ë ¥ PIL ì´ë¯¸ì§€
        save_dir: ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        use_v2: v2 í•¨ìˆ˜ ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        Dict containing pipeline results and statistics
    """
    import os
    import matplotlib.pyplot as plt
    
    os.makedirs(save_dir, exist_ok=True)
    
    debug_info = {
        'input_size': (pil_image.width, pil_image.height),
        'steps': {}
    }
    
    print("ğŸ” í…ìŠ¤íŠ¸ í¬ë¡­ íŒŒì´í”„ë¼ì¸ ë””ë²„ê·¸ ì‹œì‘...")
    
    # Step 1: ìˆ˜í‰ì„  ê²€ì¶œ
    print("  ë‹¨ê³„ 1: ìˆ˜í‰ì„  ê²€ì¶œ...")
    if use_v2:
        line_contours = enhance_and_find_contours_for_lines_v2(pil_image)
        method = "v2"
    else:
        line_contours = enhance_and_find_contours_for_lines(pil_image)
        method = "v1"
    
    debug_info['steps']['line_detection'] = {
        'method': method,
        'lines_found': len(line_contours),
        'lines': line_contours
    }
    
    # ìˆ˜í‰ì„  ì‹œê°í™” ì €ì¥
    img_with_lines = np.array(pil_image.copy())
    for x, y, w, h in line_contours:
        cv2.rectangle(img_with_lines, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(img_with_lines, f"y:{y}", (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
    
    plt.figure(figsize=(12, 8))
    plt.imshow(img_with_lines)
    plt.title(f'ìˆ˜í‰ì„  ê²€ì¶œ ê²°ê³¼ ({method}): {len(line_contours)}ê°œ ë¼ì¸')
    plt.axis('off')
    plt.savefig(os.path.join(save_dir, '1_line_detection.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # Step 2: ë¼ì¸ë³„ í¬ë¡­
    print("  ë‹¨ê³„ 2: ë¼ì¸ë³„ í¬ë¡­...")
    line_cropped_list = crop_between_lines(pil_image, line_contours)
    
    debug_info['steps']['line_cropping'] = {
        'lines_cropped': len(line_cropped_list),
        'crop_info': []
    }
    
    # ê° ë¼ì¸ í¬ë¡­ ì €ì¥
    for idx, line_crop_data in enumerate(line_cropped_list):
        line_image = line_crop_data['image_obj']
        y_top = line_crop_data['y_top_in_area']
        y_bottom = line_crop_data['y_bottom_in_area']
        
        crop_info = {
            'index': idx,
            'y_range': (y_top, y_bottom),
            'height': y_bottom - y_top,
            'size': (line_image.width, line_image.height)
        }
        debug_info['steps']['line_cropping']['crop_info'].append(crop_info)
        
        # ë¼ì¸ ì´ë¯¸ì§€ ì €ì¥
        line_save_path = os.path.join(save_dir, f'2_line_{idx:02d}_y{y_top}-{y_bottom}.png')
        line_image.save(line_save_path)
    
    # Step 3: ê° ë¼ì¸ì—ì„œ í…ìŠ¤íŠ¸ ê²€ì¶œ ë° í¬ë¡­
    print("  ë‹¨ê³„ 3: í…ìŠ¤íŠ¸ ê²€ì¶œ ë° í¬ë¡­...")
    all_text_crops = []
    total_text_crops = 0
    
    debug_info['steps']['text_cropping'] = {
        'lines_processed': 0,
        'total_text_crops': 0,
        'line_details': []
    }
    
    for line_idx, line_crop_data in enumerate(line_cropped_list):
        line_image = line_crop_data['image_obj']
        
        # í…ìŠ¤íŠ¸ ì»¨íˆ¬ì–´ ê²€ì¶œ
        text_contours = preprocess_line_image_for_text_contours(line_image)
        
        # í…ìŠ¤íŠ¸ í¬ë¡­
        text_crops_in_line = merge_contours_and_crop_text_pil(line_image, text_contours)
        
        line_detail = {
            'line_index': line_idx,
            'contours_found': len(text_contours),
            'text_crops': len(text_crops_in_line),
            'crop_details': []
        }
        
        # ë¼ì¸ë³„ í…ìŠ¤íŠ¸ í¬ë¡­ ê²°ê³¼ ì‹œê°í™”
        fig, axes = plt.subplots(2, max(1, len(text_crops_in_line)), figsize=(4*max(1, len(text_crops_in_line)), 8))
        if len(text_crops_in_line) == 1:
            axes = axes.reshape(-1, 1)
        elif len(text_crops_in_line) == 0:
            axes = axes.reshape(-1, 1)
        
        # ì›ë³¸ ë¼ì¸ ì´ë¯¸ì§€ (ìƒë‹¨)
        for col in range(max(1, len(text_crops_in_line))):
            if col == 0:
                axes[0, col].imshow(line_image)
                axes[0, col].set_title(f'Line {line_idx} Original')
            else:
                axes[0, col].axis('off')
        
        # ê°œë³„ í…ìŠ¤íŠ¸ í¬ë¡­ë“¤ (í•˜ë‹¨)
        for text_idx, text_crop_data in enumerate(text_crops_in_line):
            text_crop_image = text_crop_data['image_obj']
            x_in_line = text_crop_data['x_in_line']
            
            crop_detail = {
                'text_index': text_idx,
                'x_position': x_in_line,
                'size': (text_crop_image.width, text_crop_image.height)
            }
            line_detail['crop_details'].append(crop_detail)
            
            if text_idx < axes.shape[1]:
                axes[1, text_idx].imshow(text_crop_image)
                axes[1, text_idx].set_title(f'Text {text_idx}\nx:{x_in_line}')
                axes[1, text_idx].axis('off')
            
            # ê°œë³„ í…ìŠ¤íŠ¸ í¬ë¡­ ì €ì¥
            text_save_path = os.path.join(save_dir, f'3_line{line_idx:02d}_text{text_idx:02d}_x{x_in_line}.png')
            text_crop_image.save(text_save_path)
        
        # ë‚¨ì€ subplotë“¤ ìˆ¨ê¸°ê¸°
        for text_idx in range(len(text_crops_in_line), axes.shape[1]):
            axes[1, text_idx].axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, f'3_line_{line_idx:02d}_text_crops.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        debug_info['steps']['text_cropping']['line_details'].append(line_detail)
        total_text_crops += len(text_crops_in_line)
        all_text_crops.extend(text_crops_in_line)
    
    debug_info['steps']['text_cropping']['lines_processed'] = len(line_cropped_list)
    debug_info['steps']['text_cropping']['total_text_crops'] = total_text_crops
    
    # ìš”ì•½ ì •ë³´ ì¶œë ¥
    print(f"\nğŸ“Š ë””ë²„ê·¸ ê²°ê³¼ ìš”ì•½:")
    print(f"  ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {debug_info['input_size']}")
    print(f"  ìˆ˜í‰ì„  ê²€ì¶œ: {debug_info['steps']['line_detection']['lines_found']}ê°œ")
    print(f"  ë¼ì¸ í¬ë¡­: {debug_info['steps']['line_cropping']['lines_cropped']}ê°œ")
    print(f"  í…ìŠ¤íŠ¸ í¬ë¡­: {debug_info['steps']['text_cropping']['total_text_crops']}ê°œ")
    print(f"  ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {save_dir}")
    
    return debug_info

if __name__ == "__main__":
    import os
    from pathlib import Path
    
    print("ğŸš€ image_utils.py ë””ë²„ê·¸ ë° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)
    test_image_paths = [
        "/home/jdh251425/2025_DKU_Capstone/AI/student_id_recognition/extract_student_num/test_answer/í•™ìƒ ë‹µì§€ - 7.jpg",
        "/home/jdh251425/2025_DKU_Capstone/AI/student_id_recognition/extract_student_num/test_answer/í•™ìƒ ë‹µì§€ - 1.jpg",
        "/home/jdh251425/2025_DKU_Capstone/AI/student_id_recognition/extract_student_num/test_answer/í•™ìƒ ë‹µì§€ - 9.jpg",
        # ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤...
    ]
    
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    debug_base_dir = "/tmp/image_utils_debug"
    os.makedirs(debug_base_dir, exist_ok=True)
    
    print(f"ğŸ“ ë””ë²„ê·¸ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {debug_base_dir}")
    
    for i, img_path in enumerate(test_image_paths):
        if not os.path.exists(img_path):
            print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {img_path}")
            continue
            
        print(f"\nğŸ–¼ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ {i+1}: {Path(img_path).name}")
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            pil_image = Image.open(img_path)
            print(f"   ì´ë¯¸ì§€ í¬ê¸°: {pil_image.size}")
            
            # í˜„ì¬ ì´ë¯¸ì§€ìš© ë””ë ‰í† ë¦¬ ìƒì„±
            img_debug_dir = os.path.join(debug_base_dir, f"test_{i+1}_{Path(img_path).stem}")
            os.makedirs(img_debug_dir, exist_ok=True)
            
            # 1. ìˆ˜í‰ì„  ê²€ì¶œ ì„±ëŠ¥ ë¹„êµ
            print("   ğŸ” 1. ìˆ˜í‰ì„  ê²€ì¶œ ì„±ëŠ¥ ë¹„êµ...")
            comparison_results = visualize_line_detection_comparison(
                pil_image, 
                save_path=os.path.join(img_debug_dir, "line_detection_comparison.png"),
                test_both_versions=True
            )
            
            print(f"     V1: {comparison_results['v1']['count']}ê°œ ë¼ì¸, {comparison_results['v1']['time']:.3f}ì´ˆ")
            print(f"     V2: {comparison_results['v2']['count']}ê°œ ë¼ì¸, {comparison_results['v2']['time']:.3f}ì´ˆ")
            
            # 2. ì „ì²´ í…ìŠ¤íŠ¸ í¬ë¡­ íŒŒì´í”„ë¼ì¸ ë””ë²„ê·¸
            print("   ğŸ”§ 2. í…ìŠ¤íŠ¸ í¬ë¡­ íŒŒì´í”„ë¼ì¸ ë””ë²„ê·¸...")
            
            # V1ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            print("     V1 í•¨ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸...")
            v1_debug_dir = os.path.join(img_debug_dir, "v1_pipeline")
            v1_results = debug_text_crop_pipeline(pil_image, v1_debug_dir, use_v2=False)
            
            # V2ë¡œ í…ŒìŠ¤íŠ¸
            print("     V2 í•¨ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸...")
            v2_debug_dir = os.path.join(img_debug_dir, "v2_pipeline")
            v2_results = debug_text_crop_pipeline(pil_image, v2_debug_dir, use_v2=True)
            
            # ê²°ê³¼ ë¹„êµ
            print(f"\n   ğŸ“Š íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ë¹„êµ:")
            print(f"     V1 - ë¼ì¸: {v1_results['steps']['line_detection']['lines_found']}ê°œ, "
                  f"í…ìŠ¤íŠ¸: {v1_results['steps']['text_cropping']['total_text_crops']}ê°œ")
            print(f"     V2 - ë¼ì¸: {v2_results['steps']['line_detection']['lines_found']}ê°œ, "
                  f"í…ìŠ¤íŠ¸: {v2_results['steps']['text_cropping']['total_text_crops']}ê°œ")
            
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ“ ëª¨ë“  ê²°ê³¼ëŠ” ë‹¤ìŒ ìœ„ì¹˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {debug_base_dir}")
    print("\nğŸ’¡ ì‚¬ìš©ë²•:")
    print("  - ê° test_X í´ë”ì—ì„œ ë‹¨ê³„ë³„ ê²°ê³¼ í™•ì¸")
    print("  - line_detection_comparison.png: V1 vs V2 ì„±ëŠ¥ ë¹„êµ")
    print("  - v1_pipeline/, v2_pipeline/: ê° ë²„ì „ë³„ ìƒì„¸ ë””ë²„ê·¸ ê²°ê³¼")
    print("  - 1_line_detection.png: ìˆ˜í‰ì„  ê²€ì¶œ ê²°ê³¼")
    print("  - 2_line_XX.png: ê°œë³„ ë¼ì¸ í¬ë¡­ ê²°ê³¼")
    print("  - 3_line_XX_text_crops.png: í…ìŠ¤íŠ¸ í¬ë¡­ ê²°ê³¼") 